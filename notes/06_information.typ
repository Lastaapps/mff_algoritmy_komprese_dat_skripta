#import "../definitions.typ": *

#pagebreak()

= Theoretical Limits to Lossless Data Compression

== Information Theory
Information theory, pioneered by Claude Shannon, provides a mathematical framework for quantifying information. It helps us understand the fundamental limits of data compression and communication. Key questions in this field include:
- What is information?
- Can it be measured?
- Are there theoretical limits to how much a message can be compressed?
- How close do our current compression methods get to these limits?

=== Hartley's Formula
Ralph Hartley (1928) provided an early measure of information based on the number of possible outcomes.

#info_box(title: "Hartley's Formula", [
  If an event is chosen from $n$ equally likely possibilities, the information content of that event is $log_2 n$ bits.

  For a sequence of $k$ independent events, each chosen from $n$ possibilities, the total information is $k \cdot log_2 n$ bits. The average information per element is $log_2 n$ bits.
])

=== Shannon's Formula: Entropy
Claude Shannon (1948) extended Hartley's work to situations where outcomes are not equally likely, introducing the concept of *entropy*.

#info_box(title: "Entropy $H(X)$", [
  For a discrete random variable $X$ with range $R$ and probability mass function $p(x) = P(X=x)$, its entropy is defined as:
  $ H(X) = sum_(x in R) p(x) log_2 (1 / p(x)) = - sum_(x in R) p(x) log_2 p(x) $
  The unit of entropy is typically bits (when using $log_2$).
])

#observation(title: "Justification for Entropy", [
  Entropy can be justified through:
  - An analogy to statistical mechanics (Ludwig Boltzmann).
  - An axiomatic approach, where it's shown to be the only function satisfying a set of intuitive properties.
  - Its direct relationship to coding and compression efficiency.
])

=== Basic Properties of Entropy

#theorem([
  For any discrete random variable $X$, $H(X) >= 0$. Entropy is always non-negative.
])

#theorem([
  For a random variable $X$ with a finite range $R$ of size $n$, we have:
  $ H(X) <= log_2 n $
  The equality holds if and only if all outcomes are equally likely ($p(x) = 1/n$ for all $x in R$). This signifies that uncertainty is maximized when all outcomes are equiprobable.
])

== Information and Coding Theory

In coding theory, we map symbols from a source alphabet to codewords.

- *Code $C$*: A mapping $C: R -> {0,1}^*$ for a discrete random variable $X$ with range $R$.
- *Uniquely Decodable Code*: A code $C$ is uniquely decodable if any sequence of codewords can be unambiguously decoded back into the original sequence of symbols.
- *Average Codeword Length $L(C)$*: For a discrete random variable $X$ with probability function $p(x)$ and codeword lengths $l(x) = |C(x)|$, the average codeword length is:
  $ L(C) = sum_(x in R) p(x) l(x) $

== Upper and Lower Bounds on Codeword Length

=== Kraft-McMillan Theorem

#theorem(title: "Kraft-McMillan Inequality", [
  For any uniquely decodable code $C$ with codeword lengths $l_1, l_2, ..., l_n$, the following inequality must hold:
  $ sum_(i=1)^n 2^(-l_i) <= 1 $
  Conversely, if a set of natural numbers $l_1, l_2, ..., l_n$ satisfies this inequality, then a prefix code (which is always uniquely decodable) with these codeword lengths can be constructed.
])

#proof([
  The proof for the "if" part (existence of a prefix code) relies on constructing a binary tree where each codeword corresponds to a path from the root. The inequality ensures that there's enough "space" in the tree to assign all codewords without one being a prefix of another.

  The "only if" part (uniquely decodable codes satisfy the inequality) can be shown by considering concatenations of codewords. If the inequality were violated, an infinite number of codewords would exist for a finite length, which would prevent unique decodability.
])

=== Shannon's Source Coding Theorem

#theorem([
  Let $C$ be a uniquely decodable code for a random variable $X$. Then the average codeword length $L(C)$ is bounded by the entropy $H(X)$ as follows:
  $ H(X) <= L(C) $
  This means that, on average, a symbol cannot be compressed to fewer bits than its information content (entropy).
])

#theorem([
  For an arbitrary optimal prefix code $C$ for a random variable $X$, its average codeword length $L(C)$ satisfies:
  $ H(X) <= L(C) < H(X) + 1 $

  This implies that Huffman codes, which are optimal prefix codes, achieve an average codeword length very close to the theoretical minimum given by entropy. The "extra" bit comes from the fact that codeword lengths must be integers.
])

== Analysis of Arithmetic Coding

For arithmetic coding, the bounds on the average codeword length are slightly different.

#proposition([
  If $C$ is a prefix code generated by arithmetic coding for a random variable $X$, its average codeword length $L(C)$ satisfies:
  $ H(X) <= L(C) < H(X) + 2 $
  This bound is slightly looser than for Huffman coding, primarily due to the way arithmetic coding approximates real numbers with finite precision and sometimes needs to output an extra bit for renormalization.
])

== Probabilistic Models
The effectiveness of any statistical compression algorithm, including arithmetic coding, heavily depends on the accuracy of its probabilistic model.

- For a source generating independent and identically distributed (i.i.d.) random variables, the entropy $H(X)$ accurately reflects the average information per symbol.
- When symbols are dependent (e.g., in natural language, where the probability of a letter depends on the preceding letters), more sophisticated models are needed. These often involve estimating conditional probabilities, like $p(x_(i) | x_1 x_2 ... x_(i-1))$.
- A common approach is to use a *finite context model*, where the probability of a symbol depends only on a limited number of preceding symbols: $p(x_(i) | x_(i-k) ... x_(i-1))$.

== Problems

#task(title: "Entropy of a Discrete Random Vector", [
  *Question*: Define the entropy for a discrete random vector. \
  *Solution*: The entropy of a discrete random vector $X = (X_1, X_2, ..., X_n)$ is the joint entropy, which is calculated over the joint probability distribution $p(x_1, x_2, ..., x_n)$:
  $ H(X) = - sum_(x_1, ..., x_n) p(x_1, ..., x_n) log_2 p(x_1, ..., x_n) $
])

#task(title: "Entropy of Independent Random Variables", [
  *Question*: Prove or disprove: For an n-dimensional random vector $X=(X_1,...,X_n)$ whose components are independent random variables with the same probability distribution, we have $H(X) = n \cdot H(X_i)$. \
  *Solution*: The statement is true.
  - Since the variables are independent, the joint probability is the product of the individual probabilities: $p(x_1, ..., x_n) = p(x_1)p(x_2)...p(x_n)$.
  - Substituting this into the entropy formula:
    $ H(X) = - sum_(x_1, ..., x_n) (p(x_1)...p(x_n)) log_2 (p(x_1)...p(x_n)) $
  - Using the property $log(a b) = log(a) + log(b)$:
    $
      H(X) = - sum_(x_1, ..., x_n) (p(x_1)...p(x_n)) (log_2 p(x_1) + ... + log_2 p(x_n))
    $
  - This can be split into $n$ separate sums. For each component $X_i$, the sum over all other variables $X_j$ (where $j != i$) will be 1.
  - This leaves us with the sum of the individual entropies: $H(X) = H(X_1) + H(X_2) + ... + H(X_n)$.
  - Since all components have the same probability distribution, $H(X_i)$ is the same for all $i$. Therefore, $H(X) = n \cdot H(X_i)$.
])
